{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9t_IPFa90cs"
      },
      "source": [
        "* If you're running this on Google Colab, please uncomment and run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qh9oMcB90ct",
        "outputId": "48acc1ca-ce20-4cee-9c7a-c573d2b03205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (0.2.4)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.10/dist-packages (from optax) (0.1.88)\n",
            "Requirement already satisfied: jax>=0.4.27 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.4.27 in /usr/local/lib/python3.10/dist-packages (from optax) (0.4.33)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from optax) (1.26.4)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax) (1.11.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.87->optax) (4.12.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.87->optax) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.27->optax) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.27->optax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.27->optax) (1.13.1)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.8.5)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax) (1.26.4)\n",
            "Requirement already satisfied: jax>=0.4.27 in /usr/local/lib/python3.10/dist-packages (from flax) (0.4.33)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.71)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.27->flax) (0.4.33)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.27->flax) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.27->flax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.27->flax) (1.13.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.88)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.10/dist-packages (from optax->flax) (1.11.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (4.25.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (4.11.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.87->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.21.0)\n"
          ]
        }
      ],
      "source": [
        " !pip install optax\n",
        " !pip install flax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KoC-hLN4Oliv"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import time\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange\n",
        "from jax import jvp, value_and_grad, vmap, lax\n",
        "from flax import linen as nn\n",
        "from typing import Sequence\n",
        "from functools import partial\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOqc5OoSN_5M"
      },
      "source": [
        "## 1. SPINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3lmf86_ON_5N"
      },
      "outputs": [],
      "source": [
        "# forward function\n",
        "class SPINN(nn.Module):\n",
        "    \"\"\"\n",
        "    def input_encoding(self, t, x, y, L_x=1, L_y=1, M_x=1, M_y=1, M_t=1):\n",
        "        w_x = 2.0 * jnp.pi / L_x\n",
        "        w_y = 2.0 * jnp.pi / L_y\n",
        "        k_x = jnp.arange(1, M_x + 1)\n",
        "        k_y = jnp.arange(1, M_y + 1)\n",
        "        k_xx, k_yy = jnp.meshgrid(k_x, k_y)\n",
        "        k_xx = k_xx.flatten()\n",
        "        k_yy = k_yy.flatten()\n",
        "        k_t = jnp.power(10.0, jnp.arange(0, M_t + 1))\n",
        "        out = jnp.hstack([1,  # Ensure scalar is wrapped as an array\n",
        "            k_t * t,\n",
        "            jnp.cos(k_x * w_x * x), jnp.cos(k_y * w_y * y),\n",
        "            jnp.sin(k_x * w_x * x), jnp.sin(k_y * w_y * y),\n",
        "            jnp.cos(k_xx * w_x * x) * jnp.cos(k_yy * w_y * y),\n",
        "            jnp.cos(k_xx * w_x * x) * jnp.sin(k_yy * w_y * y),\n",
        "            jnp.sin(k_xx * w_x * x) * jnp.cos(k_yy * w_y * y),\n",
        "            jnp.sin(k_xx * w_x * x) * jnp.sin(k_yy * w_y * y)\n",
        "        ])\n",
        "        return out\n",
        "\n",
        "    features: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, y, z):\n",
        "        outputs = []\n",
        "        input_list = []\n",
        "        for i in range(len(x)):\n",
        "            input = self.input_encoding(x[i], y[i], z[i])  # Encode input\n",
        "            input_list.append(input)  # Add to the list\n",
        "\n",
        "        # Stack all inputs along a new axis (e.g., axis=0 for batch dimension)\n",
        "        inputs = jnp.stack(input_list, axis=0)\n",
        "        print(\"inputs encoding shape: \", inputs.shape)\n",
        "\n",
        "        init = nn.initializers.glorot_normal()\n",
        "        for j in range(inputs.shape[1]):\n",
        "            X = inputs[:, j].reshape(-1, 1)\n",
        "            print(\"X shape before\", X.shape)\n",
        "            for fs in self.features[:-1]:\n",
        "                X = nn.Dense(fs, kernel_init=init)(X)\n",
        "                X = jax.nn.tanh(X)\n",
        "            X = nn.Dense(self.features[-1], kernel_init=init)(X)\n",
        "            print(\"X shape after\", X.shape)\n",
        "            outputs+=[jnp.transpose(X, (1, 0))]\n",
        "\n",
        "        # Stack the outputs into a tensor\n",
        "\n",
        "        result = outputs[0]\n",
        "        for output in outputs[1:]:\n",
        "            result = jnp.einsum('...a,...b->...ab', result, output)\n",
        "\n",
        "        print(\"result shape\", result.shape)\n",
        "        return result\n",
        "\n",
        "    \"\"\"\n",
        "    features: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, y, z):\n",
        "        inputs, outputs = [x, y, z], []\n",
        "        #print(\"x shape\", x.shape)\n",
        "        init = nn.initializers.glorot_normal()\n",
        "        for X in inputs:\n",
        "            #print(\"X shape before\", X.shape)\n",
        "            for fs in self.features[:-1]:\n",
        "                X = nn.Dense(fs, kernel_init=init)(X)\n",
        "                X = nn.activation.tanh(X)\n",
        "            X = nn.Dense(self.features[-1], kernel_init=init)(X)\n",
        "\n",
        "            #print(\"X shape after\", X.shape)\n",
        "            outputs += [jnp.transpose(X, (1, 0))]\n",
        "        xy = jnp.einsum('fx, fy->fxy', outputs[0], outputs[1])\n",
        "        result = jnp.einsum('fxy, fz->xyz', xy, outputs[-1])\n",
        "        #print(\"result shape\", result.shape)\n",
        "        return result\n",
        "\n",
        "\n",
        "# hessian-vector product\n",
        "def hvp_fwdfwd(f, primals, tangents, return_primals=False):\n",
        "    g = lambda primals: jvp(f, (primals,), tangents)[1]\n",
        "    primals_out, tangents_out = jvp(g, primals, tangents)\n",
        "    if return_primals:\n",
        "        return primals_out, tangents_out\n",
        "    else:\n",
        "        return tangents_out\n",
        "\n",
        "\n",
        "# loss function\n",
        "def spinn_loss_klein_gordon3d(apply_fn, *train_data, epsilon, lam):\n",
        "    \"\"\"\n",
        "    def residual_loss(params, t, x, y, source_term):\n",
        "        # calculate u\n",
        "        u = apply_fn(params, t, x, y)\n",
        "        # tangent vector dx/dx\n",
        "        # assumes t, x, y have same shape (very important)\n",
        "        v = jnp.ones(t.shape)\n",
        "        # 2nd derivatives of u\n",
        "        utt = hvp_fwdfwd(lambda t: apply_fn(params, t, x, y), (t,), (v,))\n",
        "        uxx = hvp_fwdfwd(lambda x: apply_fn(params, t, x, y), (x,), (v,))\n",
        "        uyy = hvp_fwdfwd(lambda y: apply_fn(params, t, x, y), (y,), (v,))\n",
        "        return jnp.mean((utt - uxx - uyy + u**2 - source_term)**2)\n",
        "\n",
        "    def residuals_and_weights(params, t, x, y, source_term, tol=epsilon):\n",
        "        # Vectorized computation of u and its derivatives\n",
        "        u = vmap(lambda t_i: apply_fn(params, t_i, x, y))(t)\n",
        "        v = jnp.ones_like(t)\n",
        "\n",
        "        # Compute all derivatives using vmap\n",
        "        utt = vmap(lambda t_i: hvp_fwdfwd(\n",
        "            lambda t: apply_fn(params, t, x, y), (t_i,), (jnp.ones_like(t_i),)\n",
        "        ))(t)\n",
        "\n",
        "        uxx = vmap(lambda t_i: hvp_fwdfwd(\n",
        "            lambda x: apply_fn(params, t_i, x, y), (x,), (jnp.ones_like(x),)\n",
        "        ))(t)\n",
        "\n",
        "        uyy = vmap(lambda t_i: hvp_fwdfwd(\n",
        "            lambda y: apply_fn(params, t_i, x, y), (y,), (jnp.ones_like(y),)\n",
        "        ))(t)\n",
        "\n",
        "        # Compute residuals for all time steps at once\n",
        "        residuals = utt - uxx - uyy + u**2 - source_term\n",
        "\n",
        "        # Compute loss at each time step\n",
        "        L_t = jnp.mean(residuals**2, axis=1)\n",
        "\n",
        "        # Create matrix M for cumulative sum (lower triangular)\n",
        "        n_t = L_t.shape[0]\n",
        "        M = jnp.tril(jnp.ones((n_t, n_t)))\n",
        "\n",
        "        # Compute weights using matrix multiplication\n",
        "        W = lax.stop_gradient(jnp.exp(-tol * (M @ L_t)))\n",
        "        print(\"L_t = \", L_t)\n",
        "        print(\"W = \", W)\n",
        "        return L_t, W\n",
        "\n",
        "    \"\"\"\n",
        "    def causal_residual_loss(params, t, x, y, source_term, epsilon):\n",
        "    # Calculate weights\n",
        "        # Calculate causal weighted residual loss\n",
        "        n_t = t.shape[0]\n",
        "        residuals = []\n",
        "        weights = []\n",
        "        cumulative_loss = 0\n",
        "\n",
        "        # Compute residual at time step i\n",
        "        utt= hvp_fwdfwd(lambda t: apply_fn(params, t, x, y), (t,), (jnp.ones_like(t),))\n",
        "        uxx = hvp_fwdfwd(lambda x: apply_fn(params, t, x, y), (x,), (jnp.ones_like(x),))\n",
        "        uyy = hvp_fwdfwd(lambda y: apply_fn(params, t, x, y), (y,), (jnp.ones_like(y),))\n",
        "        u = apply_fn(params, t, x, y)\n",
        "        r_pred = utt - uxx - uyy + u**2 - source_term\n",
        "\n",
        "        L_t = jnp.mean(r_pred**2, axis = 1)\n",
        "\n",
        "        M = np.triu(np.ones((n_t, n_t)), k=1).T\n",
        "        W = lax.stop_gradient(jnp.exp(-epsilon*(M @L_t)))\n",
        "        # Compute loss for the current time step\n",
        "        residual_loss = jnp.mean(W * L_t)\n",
        "\n",
        "        return residual_loss\n",
        "\n",
        "    def initial_loss(params, t, x, y, u):\n",
        "        return jnp.mean((apply_fn(params, t, x, y) - u)**2)\n",
        "\n",
        "    def boundary_loss(params, t, x, y, u):\n",
        "        loss = 0.\n",
        "        for i in range(4):\n",
        "            loss += (1/4.) * jnp.mean((apply_fn(params, t[i], x[i], y[i]) - u[i])**2)\n",
        "        return loss\n",
        "\n",
        "    # unpack data\n",
        "    tc, xc, yc, uc, ti, xi, yi, ui, tb, xb, yb, ub = train_data\n",
        "\n",
        "    def total_loss(params):\n",
        "        # Compute residuals and weights\n",
        "        loss_res = causal_residual_loss(params, tc, xc, yc, uc, epsilon)\n",
        "        #print(\"loss_res shape\", loss_res.shape)\n",
        "        # Initial and boundary losses\n",
        "        loss_init = initial_loss(params, ti, xi, yi, ui)\n",
        "        #print(\"loss_init shape\", loss_init.shape)\n",
        "\n",
        "        loss_bound = boundary_loss(params, tb, xb, yb, ub)\n",
        "        #print(\"loss_bound shape\", loss_bound.shape)\n",
        "        # Total loss\n",
        "        return loss_res + lam*loss_init + loss_bound\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "# optimizer step function\n",
        "@partial(jax.jit, static_argnums=(0,))\n",
        "def update_model(optim, gradient, params, state):\n",
        "    updates, state = optim.update(gradient, state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3OErz7bN_5O"
      },
      "source": [
        "## 2. Data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VVY7wtfBN_5O"
      },
      "outputs": [],
      "source": [
        "# 2d time-dependent klein-gordon exact u\n",
        "def _klein_gordon3d_exact_u(t, x, y):\n",
        "    return (x + y) * jnp.cos(2*t) + (x * y) * jnp.sin(2*t)\n",
        "\n",
        "\n",
        "# 2d time-dependent klein-gordon source term\n",
        "def _klein_gordon3d_source_term(t, x, y):\n",
        "    u = _klein_gordon3d_exact_u(t, x, y)\n",
        "    return u**2 - 4*u\n",
        "\n",
        "\n",
        "# train data\n",
        "def spinn_train_generator_klein_gordon3d(nc, key):\n",
        "    keys = jax.random.split(key, 3)\n",
        "    # collocation points\n",
        "    tc = jax.random.uniform(keys[0], (nc, 1), minval=0., maxval=10.)\n",
        "    xc = jax.random.uniform(keys[1], (nc, 1), minval=-1., maxval=1.)\n",
        "    yc = jax.random.uniform(keys[2], (nc, 1), minval=-1., maxval=1.)\n",
        "    tc_mesh, xc_mesh, yc_mesh = jnp.meshgrid(tc.ravel(), xc.ravel(), yc.ravel(), indexing='ij')\n",
        "    uc = _klein_gordon3d_source_term(tc_mesh, xc_mesh, yc_mesh)\n",
        "    # initial points\n",
        "    ti = jnp.zeros((1, 1))\n",
        "    xi = xc\n",
        "    yi = yc\n",
        "    ti_mesh, xi_mesh, yi_mesh = jnp.meshgrid(ti.ravel(), xi.ravel(), yi.ravel(), indexing='ij')\n",
        "    ui = _klein_gordon3d_exact_u(ti_mesh, xi_mesh, yi_mesh)\n",
        "    # boundary points (hard-coded)\n",
        "    tb = [tc, tc, tc, tc]\n",
        "    xb = [jnp.array([[-1.]]), jnp.array([[1.]]), xc, xc]\n",
        "    yb = [yc, yc, jnp.array([[-1.]]), jnp.array([[1.]])]\n",
        "    ub = []\n",
        "    for i in range(4):\n",
        "        tb_mesh, xb_mesh, yb_mesh = jnp.meshgrid(tb[i].ravel(), xb[i].ravel(), yb[i].ravel(), indexing='ij')\n",
        "        ub += [_klein_gordon3d_exact_u(tb_mesh, xb_mesh, yb_mesh)]\n",
        "    return tc, xc, yc, uc, ti, xi, yi, ui, tb, xb, yb, ub\n",
        "\n",
        "\n",
        "# test data\n",
        "def spinn_test_generator_klein_gordon3d(nc_test):\n",
        "    t = jnp.linspace(0, 10, nc_test)\n",
        "    x = jnp.linspace(-1, 1, nc_test)\n",
        "    y = jnp.linspace(-1, 1, nc_test)\n",
        "    t = jax.lax.stop_gradient(t)\n",
        "    x = jax.lax.stop_gradient(x)\n",
        "    y = jax.lax.stop_gradient(y)\n",
        "    tm, xm, ym = jnp.meshgrid(t, x, y, indexing='ij')\n",
        "    u_gt = _klein_gordon3d_exact_u(tm, xm, ym)\n",
        "    t = t.reshape(-1, 1)\n",
        "    x = x.reshape(-1, 1)\n",
        "    y = y.reshape(-1, 1)\n",
        "    return t, x, y, u_gt, tm, xm, ym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEWeH3ZFN_5P"
      },
      "source": [
        "## 3. Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cLX1oaDUN_5P"
      },
      "outputs": [],
      "source": [
        "def relative_l2(u, u_gt):\n",
        "    return jnp.linalg.norm(u-u_gt) / jnp.linalg.norm(u_gt)\n",
        "\n",
        "def plot_klein_gordon3d(t, x, y, u):\n",
        "    fig = plt.figure(figsize=(6, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(t, x, y, c=u, s=0.5, cmap='seismic')\n",
        "    ax.set_title('U(t, x, y)', fontsize=20)\n",
        "    ax.set_xlabel('t', fontsize=18, labelpad=10)\n",
        "    ax.set_ylabel('x', fontsize=18, labelpad=10)\n",
        "    ax.set_zlabel('y', fontsize=18, labelpad=10)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8NKhfyTeyJs9"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q3WgLq_N_5P"
      },
      "source": [
        "## 4. Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VHtJazHuN_5Q"
      },
      "outputs": [],
      "source": [
        "def main(NC, NI, NB, NC_TEST, SEED, LR, EPOCHS, N_LAYERS, FEATURES, LOG_ITER, EPSILON, LAM):\n",
        "    # force jax to use one device\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "    os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
        "\n",
        "    # random key\n",
        "    key = jax.random.PRNGKey(SEED)\n",
        "    key, subkey = jax.random.split(key, 2)\n",
        "\n",
        "    # feature sizes\n",
        "    feat_sizes = tuple(FEATURES for _ in range(N_LAYERS))\n",
        "\n",
        "    # make & init model\n",
        "    model = SPINN(feat_sizes)\n",
        "    params = model.init(subkey, jnp.ones((NC, 1)), jnp.ones((NC, 1)), jnp.ones((NC, 1)))\n",
        "\n",
        "    # optimizer\n",
        "    optim = optax.adam(LR)\n",
        "    state = optim.init(params)\n",
        "\n",
        "    # dataset\n",
        "    key, subkey = jax.random.split(key, 2)\n",
        "    train_data = spinn_train_generator_klein_gordon3d(NC, subkey)\n",
        "    t, x, y, u_gt, tm, xm, ym = spinn_test_generator_klein_gordon3d(NC_TEST)\n",
        "\n",
        "    # forward & loss function\n",
        "    apply_fn = jax.jit(model.apply)\n",
        "    loss_fn = spinn_loss_klein_gordon3d(apply_fn, *train_data, epsilon = EPSILON, lam = LAM)\n",
        "\n",
        "    @jax.jit\n",
        "    def train_one_step(params, state):\n",
        "        # compute loss and gradient\n",
        "        loss, gradient = value_and_grad(loss_fn)(params)\n",
        "        # update state\n",
        "        params, state = update_model(optim, gradient, params, state)\n",
        "        return loss, params, state\n",
        "\n",
        "    # training\n",
        "    loss_log = np.array([])\n",
        "    error_log = np.array([])\n",
        "\n",
        "    start = time.time()\n",
        "    for e in trange(1, EPOCHS+1):\n",
        "        # single run\n",
        "        loss, params, state = train_one_step(params, state)\n",
        "        if e % LOG_ITER == 0:\n",
        "            u = apply_fn(params, t, x, y)\n",
        "\n",
        "            loss_log = np.append(loss_log, loss)\n",
        "\n",
        "            error = relative_l2(u, u_gt)\n",
        "\n",
        "            error_log = np.append(error_log, error)\n",
        "\n",
        "            print(f'Epoch: {e}/{EPOCHS} --> loss: {loss:.8f}, error: {error:.8f}')\n",
        "    end = time.time()\n",
        "    print(f'Runtime: {((end-start)/EPOCHS*1000):.2f} ms/iter.')\n",
        "\n",
        "    print('Solution:')\n",
        "\n",
        "    np.save(f\"loss_log.npy\", loss_log)\n",
        "    np.save(\"error_log.npy\", error_log)\n",
        "\n",
        "    u = apply_fn(params, t, x, y)\n",
        "    plot_klein_gordon3d(tm, xm, ym, u)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdzoogAN_5Q"
      },
      "source": [
        "## 5. Run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-DGXwqYN_5Q",
        "outputId": "d99a728b-d3fd-417b-ffb4-c77d7ecbddf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 135/5000 [00:11<02:41, 30.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 100/5000 --> loss: 7.36400366, error: 1.00068188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▍         | 234/5000 [00:16<02:54, 27.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 200/5000 --> loss: 6.52692032, error: 0.98213446\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 335/5000 [00:20<02:09, 35.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 300/5000 --> loss: 6.14153576, error: 0.97260010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 435/5000 [00:23<02:08, 35.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 400/5000 --> loss: 5.90241337, error: 0.96486682\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 535/5000 [00:27<02:06, 35.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 500/5000 --> loss: 5.74668550, error: 0.96326286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 635/5000 [00:32<02:24, 30.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 600/5000 --> loss: 5.62800694, error: 0.97067118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▍        | 735/5000 [00:35<01:59, 35.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 700/5000 --> loss: 5.39012432, error: 0.98729521\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 835/5000 [00:39<01:53, 36.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 800/5000 --> loss: 5.16672611, error: 0.96125859\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▊        | 934/5000 [00:43<02:32, 26.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 900/5000 --> loss: 5.07778120, error: 0.99210238\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 1035/5000 [00:47<01:50, 35.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1000/5000 --> loss: 4.92305279, error: 1.06662786\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 1135/5000 [00:51<01:45, 36.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1100/5000 --> loss: 4.71471024, error: 1.09404588\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▍       | 1235/5000 [00:54<01:46, 35.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1200/5000 --> loss: 5.63543510, error: 1.06288469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 1335/5000 [00:59<01:56, 31.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1300/5000 --> loss: 4.14161015, error: 1.01593637\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▊       | 1435/5000 [01:03<01:38, 36.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1400/5000 --> loss: 3.96455979, error: 0.93565112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 1535/5000 [01:06<01:35, 36.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1500/5000 --> loss: 3.81172442, error: 0.87791556\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1634/5000 [01:11<02:09, 25.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1600/5000 --> loss: 3.65430331, error: 0.86402243\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▍      | 1735/5000 [01:15<01:31, 35.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1700/5000 --> loss: 3.53450322, error: 0.85979337\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 1835/5000 [01:18<01:30, 35.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1800/5000 --> loss: 3.43544507, error: 0.91424441\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▊      | 1934/5000 [01:22<01:35, 32.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1900/5000 --> loss: 3.29237628, error: 0.96662337\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 2035/5000 [01:27<01:25, 34.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2000/5000 --> loss: 3.12290549, error: 1.12497818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 2099/5000 [01:30<01:49, 26.44it/s]"
          ]
        }
      ],
      "source": [
        "main(NC=64, NI=64, NB=64, NC_TEST=100, SEED=444, LR=1e-3, EPOCHS=50000, N_LAYERS=4, FEATURES=64, LOG_ITER=100, EPSILON = 1e-3, LAM = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TDBqT8C90cw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
